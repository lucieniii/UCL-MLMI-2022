{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MPHY0041 - CW1 - Notebook"
      ],
      "metadata": {
        "id": "qu6qajUhItZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import several libraries\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import expit"
      ],
      "metadata": {
        "id": "7hUWuuvhJiZy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Decision Boundaries"
      ],
      "metadata": {
        "id": "UL41pCQvI2An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Classifier Implementations"
      ],
      "metadata": {
        "id": "KhvGJtC8I69j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Complete the code for Rosenblatt's Perceptron"
      ],
      "metadata": {
        "id": "5dU_aaePJXiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#implement rosenblatt's perceptron for a 2D example\n",
        "#Y are the target labels (-1,1)\n",
        "#X is the input data\n",
        "#rho is the learning rate\n",
        "#max_iter controls the maximum number of iterations\n",
        "##Note: both, rho and max_iter can be left to their default values\n",
        "\n",
        "#function returns the intercept_ (\\beta_0) and the coefficients_ (\\beta_i)\n",
        "def rosen_perceptron(Y, X, rho=0.1, max_iter = 1000):\n",
        "  #initialize with random parameters\n",
        "  intercept_ = np.random.randn()\n",
        "  #as many coefficients as there are columns (features) in X\n",
        "  coef_ = np.random.randn(X.shape[1])\n",
        "  \n",
        "  #make predictions\n",
        "  Y_hat = intercept_ + np.dot(X, coef_)  \n",
        "  \n",
        "  #compute the list of misclassified samples and store in M  \n",
        "  M = \n",
        "\n",
        "  iter = 0\n",
        "  #as long as there are misclassified samples in M and we have not reached the\n",
        "  #maximal number of interations continue to update coefficients etc.\n",
        "  while len(M) > 0 and iter < max_iter:\n",
        "\n",
        "    #make gradient step (update intercept and coefficients)\n",
        "    intercept_  =\n",
        "    coef_       = \n",
        "\n",
        "    #make predictions using new parameters\n",
        "    Y_hat = intercept_ + np.dot(X, coef_)\n",
        "\n",
        "    ##update the list of misclassified samples and store in M  \n",
        "    M = \n",
        "\n",
        "    iter += 1\n",
        "  \n",
        "  return intercept_, coef_"
      ],
      "metadata": {
        "id": "WkEw05EzJWw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Complete the code for LogisticRegression via Gradient Descent"
      ],
      "metadata": {
        "id": "bjdulpToM1nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the sigmoid function\n",
        "def sigmoid(value):\n",
        "    return expit(value)"
      ],
      "metadata": {
        "id": "HN-UF2-8NB0B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function fits Logistic Regression using Gradient Descent\n",
        "#inputs are\n",
        "#y:         target labels (0,1)\n",
        "#features:  input feature matrix\n",
        "#eps:       tolerance for the stopping criterion\n",
        "#rho:       step size in gradient descent\n",
        "\n",
        "#this function returns the beta coefficients for a LogisticRegression model\n",
        "#the first entry (betas[0] is the intercept)\n",
        "def fit_LogReg_GRAD(y, features, eps=0.00000000001, rho=0.01):\n",
        "    #sample size\n",
        "    N = features.shape[0]\n",
        "    #feature dim\n",
        "    p = features.shape[1] + 1\n",
        "    #initalize beta with 0s\n",
        "    betas = np.zeros(p)\n",
        "\n",
        "    #add column of 1 to X\n",
        "    X = np.c_[np.ones(N),features]\n",
        "    #compute predictions for class 1\n",
        "    prob1 = sigmoid( X @ betas)\n",
        "    #compute predictions for class 2\n",
        "    prob0 = 1.0 - prob1\n",
        "\n",
        "    old_cost = 100000000\n",
        "    #compute the cost (J) of the current solution\n",
        "    #COMPLETE THIS LINE\n",
        "    cost = \n",
        "    #as long as the there is a differentce between old and new cost that is larger\n",
        "    #than the tolerance we update the parameters\n",
        "    while np.abs(old_cost - cost) > eps:\n",
        "        #beta update (one step of the gradient descent)\n",
        "        #COMPLETE THIS LINE\n",
        "        betas =\n",
        "\n",
        "        #update probabilities\n",
        "        prob1 = sigmoid( X @ betas)\n",
        "        prob0 = 1.0 - prob1\n",
        "        \n",
        "        #deal with 'underflow'\n",
        "        prob1[prob1 == 0] = 0.000001\n",
        "        prob0[prob0 == 0] = 0.000001\n",
        "\n",
        "        old_cost = cost\n",
        "        #recompute cost\n",
        "        #COMPLETE THIS LINE\n",
        "        cost = \n",
        "\n",
        "        #reduce learning rate in case the cost increased\n",
        "        if cost > old_cost:\n",
        "          rho /= 2\n",
        "    \n",
        "    return(betas)"
      ],
      "metadata": {
        "id": "ijOsUrBwNSPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Complete the code for HingeLoss with L2 regularization using Gradient Descent"
      ],
      "metadata": {
        "id": "CaktByf5PEGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function fits a linear classifier with Hinge Loss and L2 regularization using Gradient Descent\n",
        "#inputs are\n",
        "#y:         target labels (0,1)\n",
        "#features:  input feature matrix\n",
        "#lam:       the regularization parameter \\lambda\n",
        "#eps:       tolerance for the stopping criterion\n",
        "#rho:     step size in gradient descent\n",
        "#max_iter:  maximum number of iterations the algorithm is run\n",
        "\n",
        "#this function returns the beta coefficients for a HingeL2 linear model\n",
        "#the first entry (betas[0] is the intercept)\n",
        "\n",
        "def fit_HingeL2_GRAD(y, features, lam=1, eps=0.00000000001, rho=0.01, max_iter=1000):\n",
        "    #sample size\n",
        "    N = features.shape[0]\n",
        "    #feature dim\n",
        "    p = features.shape[1] + 1\n",
        "    #initalize beta random values  \n",
        "    betas = np.random.normal(size=p)\n",
        "\n",
        "    #add column of 1 to X\n",
        "    X = np.c_[np.ones(N),features]\n",
        "    #prediction, using a linear model\n",
        "    yhat = X @ betas\n",
        "  \n",
        "    #initialize old_cost with a very high value\n",
        "    old_cost = 100000000\n",
        "    #compute the cost (J) of the current solution\n",
        "    #COMPLETE THIS \n",
        "    cost = \n",
        "      \n",
        "    epoch=0\n",
        "    while epoch < max_iter and np.abs(old_cost - cost) > eps:\n",
        "        epoch += 1\n",
        "        #beta update (one step of the gradient descent)\n",
        "        #COMPLETE THIS \n",
        "        betas =\n",
        "\n",
        "        #update prediction\n",
        "        yhat = X @ betas    \n",
        "        \n",
        "        old_cost = cost\n",
        "        #recompute cost\n",
        "        #COMPLETE THIS (same as above)        \n",
        "        cost = \n",
        "\n",
        "        #reduce learning rate in case the cost increased\n",
        "        if cost > old_cost:\n",
        "          rho /= 2\n",
        "    return(betas)"
      ],
      "metadata": {
        "id": "uyBG7VbbPg4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Suggest improvements to the ML pipeline"
      ],
      "metadata": {
        "id": "7mFcrcMxJBnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   \n",
        "2.  \n",
        "3. \n",
        "4. \n",
        "\n"
      ],
      "metadata": {
        "id": "5Jl2NRI7Sy9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Vessel Segmentation"
      ],
      "metadata": {
        "id": "LwlddBbwJMT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import ensemble\n",
        "from sklearn import svm\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction import image\n",
        "from skimage.filters import scharr\n",
        "from skimage.color import rgb2gray\n"
      ],
      "metadata": {
        "id": "MTWon3DDJLal"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = ''\n",
        "vali_path = ''\n",
        "test_path = ''"
      ],
      "metadata": {
        "id": "-Rs2wpB8TYjk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function takes an input image and\n",
        "#does the manual feature extraction\n",
        "#color image? grayscale? just one channel? apply any filters?\n",
        "#if the variable nchan is sset to True, then the return is just the number of channels\n",
        "\n",
        "def preprocess_img(img, nchan=False):\n",
        "    #some examples for pre-processing:\n",
        "\n",
        "    ##E.g., do nothing\n",
        "    ##copy all three channels\n",
        "    #res = img\n",
        "\n",
        "    ##use just the green channel\n",
        "    #res = img[:,:,1]  \n",
        "\n",
        "    ##single channel gray    \n",
        "    res = rgb2gray(img) * 255\n",
        "\n",
        "    #add some edge filter applied to the gray-scale image\n",
        "    res = np.dstack((res, scharr(rgb2gray(img)) * 255))\n",
        "    \n",
        "    #if nchan is True, then just return the number of channels\n",
        "    #the pre-processing produces\n",
        "    if nchan:\n",
        "        try:\n",
        "            return(res.shape[2])\n",
        "        except IndexError:\n",
        "            return(1)\n",
        "    return (res)"
      ],
      "metadata": {
        "id": "EzWPu711Tbff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#turn the images into a training dataset\n",
        "\n",
        "#train_path:    path to the training data\n",
        "#n_train_patch: number of patches to be extracted per image\n",
        "#ps :           patch size as integer (e.g., 9 -> 9x9 patches)\n",
        "#chan:          number of 'channels' the pre_processed image will have\n",
        "#verbose:       if True, print some updates on the processing\n",
        "\n",
        "def create_training_set(train_path, n_train_patch, ps, chan, verbose=True):\n",
        "  if verbose:\n",
        "    print(\"Extracting \" + str(n_train_patch) + \" pataches of size \" + str(ps) + \"x\" + str(ps) + \" from each image.\")\n",
        "\n",
        "  #dimension of square patch\n",
        "  patch_dim = (ps, ps)\n",
        "  #center of the patch\n",
        "  patch_cnt = int((ps-1)/2)\n",
        "  #dimension of the 'flattened' patch  (i.e., a single vector)  \n",
        "  flat_dim = ps*ps*chan\n",
        "\n",
        "  #initialize Y and X\n",
        "  Y = []\n",
        "  X = np.empty((0,flat_dim))\n",
        "\n",
        "  for s in np.arange(80):\n",
        "    if verbose:\n",
        "      print(\"Extracting data from subject: \" + str(s))\n",
        "    tmp =     train_path + 'images/' + str(s) + '.png'\n",
        "    tmp_msk = train_path + 'masks/' + str(s) + '.png'\n",
        "    \n",
        "    img_m = imageio.imread(tmp)\n",
        "    msk_x = imageio.imread(tmp_msk)\n",
        "    #binarize the mask\n",
        "    msk = (msk_x > 0) * 1\n",
        "\n",
        "    #extract features (using the function defined above)\n",
        "    frames = preprocess_img(img_m)\n",
        "\n",
        "    #add the labels as an additional channel\n",
        "    frames = np.dstack((frames, msk))\n",
        "\n",
        "    #sample patches    \n",
        "    patch = image.extract_patches_2d(frames, patch_dim, max_patches=n_train_patch)\n",
        "    \n",
        "    #each patch receives the label of the center value in the 'label' channel\n",
        "    Y_tmp = patch[:,patch_cnt,patch_cnt,chan] == 1\n",
        "\n",
        "    #turn patches into a matrix where each row corresponds to all the features\n",
        "    #of one patch\n",
        "    X_tmp = np.reshape(patch[:,:,:,0:chan],(n_train_patch,flat_dim))\n",
        "    \n",
        "    #concatenate with Y and X\n",
        "    Y.extend(Y_tmp)\n",
        "    X = np.concatenate((X, X_tmp), axis=0)\n",
        "  return (Y, X)"
      ],
      "metadata": {
        "id": "_RID6hpfT9Cr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#that is a bit clunky to get the channel size\n",
        "im = imageio.imread(train_path + 'images/0.png')\n",
        "chan = preprocess_img(im, True)"
      ],
      "metadata": {
        "id": "OwVMnJKzUvMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the input image\n",
        "plt.imshow(im)"
      ],
      "metadata": {
        "id": "OTU57_9UU9Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### code for parts a) - f)"
      ],
      "metadata": {
        "id": "ZSrGfRpdVNTw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTQEWDUeVU5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}